{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from ReplayBuffer import ReplayBuffer\n",
    "from ActorNetwork import ActorNetwork\n",
    "from CriticNetwork import CriticNetwork\n",
    "from OU import OU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcd1e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage_path = \"ddpg_v1.2\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e448c47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10000\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001     #Target Network HyperParameters\n",
    "LRA = 0.0001    #Learning rate for Actor\n",
    "LRC = 0.001     #Lerning rate for Critic\n",
    "\n",
    "'''\n",
    "Action : [\n",
    "           limitprice1 scaler belongs to [0,1]\n",
    "           limitprice2 scaler belongs to [0,1]\n",
    "         ] (Two output nuerons)\n",
    "'''\n",
    "action_dim = 2\n",
    "\n",
    "'''\n",
    "State : [\n",
    "          Proximity (1)\n",
    "          Balancing_Price (1)\n",
    "          Required_Quantity (1)\n",
    "        ] \n",
    "'''\n",
    "state_dim = 3\n",
    "\n",
    "np.random.seed(1337)\n",
    "EXPLORE = 100000.0\n",
    "\n",
    "step = 0\n",
    "epsilon = 1\n",
    "\n",
    "ou = OU()       #Ornstein-Uhlenbeck Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541f2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(\n",
    "    device_count={'GPU': 1},\n",
    "    intra_op_parallelism_threads=1,\n",
    "    allow_soft_placement=True\n",
    ")\n",
    "\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6\n",
    "\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39dc42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actor = ActorNetwork(session, state_dim, action_dim, BATCH_SIZE, TAU, LRA)\n",
    "critic = CriticNetwork(session, state_dim, action_dim, BATCH_SIZE, TAU, LRC)\n",
    "\n",
    "data_storage_path = \"/mnt/d/PowerTAC/PowerTAC2021/experiments_scripts/powertac_simulator_py/ddpg_based_wholesale_strategy_powertac/\"\n",
    "replay_buffer = pd.read_csv(data_storage_path + 'replay_buffer.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddpg_network():\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "\n",
    "        print(\"Epoch \", (epoch+1))\n",
    "        print(\"-\"*12)\n",
    "        loss = 0\n",
    "\n",
    "        #Do the batch update\n",
    "        batch = replay_buffer.sample(n=BATCH_SIZE)\n",
    "        states = np.asarray(batch[batch.columns[0:3]])\n",
    "        actions = np.asarray(batch[batch.columns[3:5]])\n",
    "        rewards = np.asarray(batch[batch.columns[5:6]])\n",
    "        new_states = np.asarray(batch[batch.columns[6:9]])\n",
    "        terminals = np.asanyarray(batch[batch.columns[9:10]])\n",
    "\n",
    "        y_t = np.zeros([BATCH_SIZE,1])\n",
    "\n",
    "        # print(\"States\", states.shape)\n",
    "        # print(\"Actions\", actions.shape)\n",
    "        # print(\"Rewards\", rewards.shape)\n",
    "        # print(\"New_States\", new_states.shape)\n",
    "\n",
    "        with session.as_default():\n",
    "            with session.graph.as_default():\n",
    "                \n",
    "                target_q_values = critic.target_model.predict([new_states, actor.target_model.predict(new_states)])\n",
    "                # print(\"Target_Q_Values\", target_q_values.shape)\n",
    "\n",
    "                for k in range(BATCH_SIZE):\n",
    "                    if terminals[k] == 1:\n",
    "                        y_t[k] = rewards[k]\n",
    "                    else:\n",
    "                        y_t[k] = rewards[k] + GAMMA*target_q_values[k]    # check by keeping negative sign here\n",
    "\n",
    "                # print(\"Bellman Rewards\", y_t)\n",
    "                loss += critic.model.train_on_batch([states,actions], y_t)\n",
    "                print(\"Loss\", loss)\n",
    "                a_for_grad = actor.model.predict(states)      # This may not be required, a_for_grad should be replaced by actions ##### Check PENDING #####\n",
    "                # print(\"a_for_grad\", a_for_grad)\n",
    "                grads = critic.gradients(states, a_for_grad)       # a_for_grad is replaced by actions ##### Check PENDING #####   shape ERROR \n",
    "                # print(\"grads\", grads)\n",
    "                actor.train(states, grads)\n",
    "                actor.target_train()\n",
    "                critic.target_train()\n",
    "\n",
    "    print(\"Training Completed !!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859287a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddpg_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf4acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_Action(states):\n",
    "\n",
    "        actions = list()\n",
    "\n",
    "        for state in states:\n",
    "\n",
    "            try:\n",
    "\n",
    "                # self.epsilon -= 1.0 / self.EXPLORE\n",
    "                # a_t = np.zeros([self.action_dim])\n",
    "                # noise_t = np.zeros([self.action_dim])\n",
    "\n",
    "                with session.as_default():\n",
    "                    with session.graph.as_default():\n",
    "\n",
    "                        a_t_original = actor.model.predict(state.reshape(1, state.shape[0]))[0].tolist()\n",
    "                        # noise_t[0] = max(self.epsilon, 0) * self.ou.function(a_t_original[0],  0.0 , 0.60, 0.30)  # decide theta, sigma and mu for limitprice\n",
    "\n",
    "                        # a_t[0] = a_t_original[0] + noise_t[0]\n",
    "                        # a_t[1] = a_t_original[1] + noise_t[1]\n",
    "\n",
    "                        # print(a_t_original)\n",
    "                        actions.append(list(a_t_original))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1cb96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = replay_buffer.sample(n=10)\n",
    "states = np.asarray(batch[batch.columns[0:3]])\n",
    "\n",
    "lps = choose_Action(states)\n",
    "\n",
    "for lp in lps:\n",
    "    print(lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "\n",
    "        with session.as_default():\n",
    "                with session.graph.as_default():        \n",
    "\n",
    "                    timestamp = int(datetime.datetime.now().timestamp())\n",
    "\n",
    "                    actor.model.save_weights(model_storage_path + \"/actormodel.h5\", overwrite=True)\n",
    "                    with open(model_storage_path + \"/actormodel.json\", \"w\") as outfile:\n",
    "                        json.dump(actor.model.to_json(), outfile)\n",
    "\n",
    "                    critic.model.save_weights(model_storage_path + \"/criticmodel.h5\", overwrite=True)\n",
    "                    with open(model_storage_path + \"/criticmodel.json\", \"w\") as outfile:\n",
    "                        json.dump(critic.model.to_json(), outfile)\n",
    "\n",
    "                    actor.target_model.save_weights(model_storage_path + \"/actortargetmodel.h5\", overwrite=True)\n",
    "                    with open(model_storage_path + \"/actormodeltarget.json\", \"w\") as outfile:\n",
    "                        json.dump(actor.target_model.to_json(), outfile)\n",
    "\n",
    "                    critic.target_model.save_weights(model_storage_path + \"/critictargetmodel.h5\", overwrite=True)\n",
    "                    with open(model_storage_path + \"/criticmodeltarget.json\", \"w\") as outfile:\n",
    "                        json.dump(critic.target_model.to_json(), outfile)\n",
    "\n",
    "                    print(\"Models Saved Successfully !!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
